{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part01_NLP_exercise_4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNH8I20zYinbJFgZTZn/Ymm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpantelis/fashion-mnist-/blob/master/part01_NLP_exercise_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koYGbAaVdhS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installing nltk\n",
        "!pip install -U nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVkqbrRfd05s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download the corpus \n",
        "import nltk\n",
        "nltk.download('genesis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2YLDN1-eLs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load twitter_samples books from corpus\n",
        "from nltk.corpus import genesis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez33h9WueSyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the files that my corpus contains\n",
        "genesis.fileids()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6eEPkBxfAOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = genesis.raw('english-web.txt')\n",
        "print(len(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZBnensAgfcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"There are\", len(sentences), \"sentences:\")\n",
        "#for sent in sentences:\n",
        "#    print(sent)\n",
        "#    print(\"_________________\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQm3YaAbhJuM",
        "colab_type": "code",
        "outputId": "bcfc2c97-5e1f-41a6-99cc-5c9c5611ec34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "\n",
        "whitespace_wt = WhitespaceTokenizer()\n",
        "\n",
        "#Αυτη είναι μια συνάρτηση που δεν την χρησιμοποιώ\n",
        "def remove_stop_wards(sentences):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  sentences_tokenized = []\n",
        "  for sent in sentences:\n",
        "      sent_tok_raw = word_tokenize(sent)\n",
        "      sent_tok_filtered = []\n",
        "      for w in sent_tok_raw: \n",
        "        if w not in stop_words: \n",
        "            sent_tok_filtered.append(w) \n",
        "      sentences_tokenized.append(sent_tok_filtered)\n",
        "  return sentences_tokenized"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egsWXR4mkZiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "#Εδώ σπάω τις λέξεις και διώχνω τελειές και και σημεία στήξης\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "sentences_tokenized = []\n",
        "for sent in sentences:\n",
        "  sent_tok = tokenizer.tokenize(sent)      \n",
        "  sentences_tokenized.append(sent_tok)\n",
        "  #print(sent_tok)\n",
        "  #print('------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOV17EqUnVP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating vocabulary\n",
        "#create a dictionary with all the words of the text, distinct, key is the word and the value is the number that the word is appeared\n",
        "vocabulary = {}\n",
        "for sent in sentences_tokenized:\n",
        "  \n",
        "  for word in sent:\n",
        "    if word not in vocabulary.keys():\n",
        "      vocabulary[word] = 1\n",
        "    else:\n",
        "      current_value = vocabulary[word]\n",
        "      vocabulary[word] = current_value + 1\n",
        "#print(vocabulary)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmK--AqbXbq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here I replace any word that is appeared less than 10 times with the 'UNK', the dictionary vovabulay_filtered contains the result, and the list unknown_words contains the unknown words\n",
        "vocabulary_filtered = {}\n",
        "unknown_words = []\n",
        "vocabulary_filtered['UNK'] = 0\n",
        "for k,v in vocabulary.items():\n",
        "  if(v >= 10):\n",
        "    vocabulary_filtered[k] = v\n",
        "  else:\n",
        "    current = vocabulary_filtered['UNK']\n",
        "    unknown_words.append(k)\n",
        "    vocabulary_filtered['UNK'] = current + v \n",
        "#print(vocabulary)\n",
        "#print(vocabulary_filtered)\n",
        "#print(unknown_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK75UzI_Zf9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here I replace every unknown word of the initial sentences and the result is saved in the list below\n",
        "sentences_tokenized_filtered = []\n",
        "for sent in sentences_tokenized:\n",
        "  sent_filtered = []\n",
        "  for word in sent:\n",
        "    if word not in unknown_words:\n",
        "      sent_filtered.append(word)\n",
        "    else:\n",
        "      sent_filtered.append('UNK')\n",
        "  sentences_tokenized_filtered.append(sent_filtered)\n",
        "#print(sentences_tokenized_filtered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LksLKasDlOVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here i create the n grams\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in sentences_tokenized_filtered:\n",
        "    \n",
        "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "#print(unigram_counter)   \n",
        "#print(bigram_counter)\n",
        "#print(trigram_counter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPAWwebtlizR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram_laplace = {}\n",
        "for bigram in list(bigram_counter):\n",
        "  #simple moodifications \n",
        "  temp_0 = (bigram[0],)\n",
        "  temp_1 = (bigram[1],)\n",
        "  prop = (bigram_counter[bigram] + 1)/(unigram_counter[temp_1] + len(vocabulary_filtered.keys()))\n",
        "\n",
        "  bigram_laplace[bigram] = prop\n",
        "#print(bigram_laplace)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}