{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part01_NLP_exercise_4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXk+x0LL1iQAx97PTtkT63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpantelis/fashion-mnist-/blob/master/part01_NLP_exercise_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koYGbAaVdhS3",
        "colab_type": "code",
        "outputId": "eb82e55d-14e9-4874-b649-601b8fba055b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Installing nltk\n",
        "!pip install -U nltk"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.4.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVkqbrRfd05s",
        "colab_type": "code",
        "outputId": "b3068908-0613-4d31-83df-ac1605b9466e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#download the corpus \n",
        "import nltk\n",
        "nltk.download('genesis')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2YLDN1-eLs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load twitter_samples books from corpus\n",
        "from nltk.corpus import genesis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez33h9WueSyk",
        "colab_type": "code",
        "outputId": "bf5a9331-7275-49c5-ddfd-fc458a83fdb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#the files that my corpus contains\n",
        "genesis.fileids()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['english-kjv.txt',\n",
              " 'english-web.txt',\n",
              " 'finnish.txt',\n",
              " 'french.txt',\n",
              " 'german.txt',\n",
              " 'lolcat.txt',\n",
              " 'portuguese.txt',\n",
              " 'swedish.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6eEPkBxfAOo",
        "colab_type": "code",
        "outputId": "eb988d76-8b14-4ee9-9dae-30a743e43520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "text = genesis.raw('english-web.txt')\n",
        "print(len(text))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "188666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZBnensAgfcz",
        "colab_type": "code",
        "outputId": "204d2d62-3d5e-4224-80e0-10032245bfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"There are\", len(sentences), \"sentences:\")\n",
        "#for sent in sentences:\n",
        "#    print(sent)\n",
        "#    print(\"_________________\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2232 sentences:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQm3YaAbhJuM",
        "colab_type": "code",
        "outputId": "9b429f4d-1c9e-41b6-f0df-22f7fcd31ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "\n",
        "whitespace_wt = WhitespaceTokenizer()\n",
        "\n",
        "#Αυτη είναι μια συνάρτηση που δεν την χρησιμοποιώ\n",
        "def remove_stop_wards(sentences):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  sentences_tokenized = []\n",
        "  for sent in sentences:\n",
        "      sent_tok_raw = word_tokenize(sent)\n",
        "      sent_tok_filtered = []\n",
        "      for w in sent_tok_raw: \n",
        "        if w not in stop_words: \n",
        "            sent_tok_filtered.append(w) \n",
        "      sentences_tokenized.append(sent_tok_filtered)\n",
        "  return sentences_tokenized"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egsWXR4mkZiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "#Εδώ σπάω τις λέξεις και διώχνω τελειές και και σημεία στήξης\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "sentences_tokenized = []\n",
        "for sent in sentences:\n",
        "  sent_tok = tokenizer.tokenize(sent)      \n",
        "  sentences_tokenized.append(sent_tok)\n",
        "  print(sent_tok)\n",
        "  print('------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yne-mW8UpY-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sentences_tokenized[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOV17EqUnVP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating vocabulary\n",
        "#create a dictionary with all the words of the text, distinct, key is the word and the value is the number that the word is appeared\n",
        "vocabulary = {}\n",
        "for sent in sentences_tokenized:\n",
        "  \n",
        "  for word in sent:\n",
        "    if word not in vocabulary.keys():\n",
        "      vocabulary[word] = 1\n",
        "    else:\n",
        "      current_value = vocabulary[word]\n",
        "      vocabulary[word] = current_value + 1\n",
        "print(vocabulary)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmK--AqbXbq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here I replace any word that is appeared less than 10 times with the 'UNK', the dictionary vovabulay_filtered contains the result, and the list unknown_words contains the unknown words\n",
        "vocabulary_filtered = {}\n",
        "unknown_words = []\n",
        "vocabulary_filtered['UNK'] = 0\n",
        "for k,v in vocabulary.items():\n",
        "  if(v >= 10):\n",
        "    vocabulary_filtered[k] = v\n",
        "  else:\n",
        "    current = vocabulary_filtered['UNK']\n",
        "    unknown_words.append(k)\n",
        "    vocabulary_filtered['UNK'] = current + v \n",
        "print(vocabulary)\n",
        "print(vocabulary_filtered)\n",
        "print(unknown_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK75UzI_Zf9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here I replace every unknown word of the initial sentences and the result is saved in the list below\n",
        "sentences_tokenized_filtered = []\n",
        "for sent in sentences_tokenized:\n",
        "  sent_filtered = []\n",
        "  for word in sent:\n",
        "    if word not in unknown_words:\n",
        "      sent_filtered.append(word)\n",
        "    else:\n",
        "      sent_filtered.append('UNK')\n",
        "  sentences_tokenized_filtered.append(sent_filtered)\n",
        "print(sentences_tokenized_filtered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LksLKasDlOVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here i create the n grams\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in sentences_tokenized_filtered:\n",
        "    \n",
        "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "print(unigram_counter)   \n",
        "print(bigram_counter)\n",
        "print(trigram_counter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPAWwebtlizR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}